{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4AXA6fRkDc3"
      },
      "source": [
        "# **Task Lab [5]_AIE425_Intelligent Recommender Systems**\n",
        "# Name: **Aser Mohamed Ali Mahmoud**\n",
        "# ID: **222102487**\n",
        "# Program: **AIS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QCncdzRkx-z"
      },
      "source": [
        "# The code adds an implementation to **Raw Cosine Similarity** and **Mean-Centered Cosine Similarity**\n",
        "\n",
        "# Also, a calculation loops with methods to calculate the **Processing Time**, **Execution Time**, **TIME it took for each Loop**, and **Space Complexity**\n",
        "\n",
        "# Ensuring that is no **Built-In Functions** or **Dot-Products**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDq6NK9FoJuU"
      },
      "source": [
        "# The Original Code Execution Summary: Clustering Algorithm Comparison\n",
        "\n",
        "This code block executes a comprehensive comparison of three fundamental clustering algorithms: K-Means, K-Nearest Neighbors (KNN), and Decision Tree, specifically tailored for a Collaborative Filtering (CF) scenario using the MovieLens 100K dataset.\n",
        "\n",
        "The primary focus is on **algorithm complexity and performance analysis**, with each custom-built model tracking its own execution time and operation count.\n",
        "\n",
        "### Execution Flow:\n",
        "\n",
        "1.  **Data Setup (Colab):** The code first handles the Colab environment by checking for and unzipping the `Movie_Lens_100K_Dataset.zip` file.\n",
        "2.  **Feature Engineering:** It loads the MovieLens `u.data` file and creates a simplified 2-feature vector for each user: `[average rating, number of ratings]`.\n",
        "3.  **Base Label Generation:** A standard `sklearn.KMeans` model is used to quickly generate a set of 5 \"base clusters.\" These are used as the \"ground truth\" labels for training the supervised models (KNN and Decision Tree).\n",
        "4.  **Training & Testing:** The data is split into training (80%) and testing (20%) sets.\n",
        "5.  **Algorithm Comparison:**\n",
        "    *   **K-Means Clustering (Custom):** Trained and tested.\n",
        "    *   **K-Nearest Neighbors (Custom):** Trained and tested.\n",
        "    *   **Decision Tree Clustering (Custom):** Trained and tested.\n",
        "6.  **Performance Reporting:** For each algorithm, the output includes:\n",
        "    *   **Actual Time:** Training and prediction time in seconds/milliseconds.\n",
        "    *   **Operation Count:** A custom counter tracking the number of fundamental arithmetic operations.\n",
        "    *   **Theoretical Complexity:** The expected Big O notation (e.g., O(n·k·d·t)) and a comparison to the actual operation count.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjwpyceqjhT3",
        "outputId": "aeb15a2f-bab4-4fa9-a465-6b39f62aa7ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found zip file at: /content/Movie_Lens_100K_Dataset.zip\n",
            "Successfully extracted to: /content/ml-100k\n",
            "Loading MovieLens dataset from u.data...\n",
            "✓ Loaded 100,000 ratings\n",
            "  Users: 943\n",
            "  Movies: 1,682\n",
            "✓ Created feature vectors for 943 users\n",
            "  Matrix shape: (943, 2)\n",
            "\n",
            "======================================================================\n",
            " GENERATING BASE CLUSTERS (SKLEARN KMEANS)\n",
            "======================================================================\n",
            "✓ Generated 5 base clusters for training KNN and DT.\n",
            "\n",
            "Data Split: Train=754 users, Test=189 users\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            " K-MEANS CLUSTERING - TRAINING\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Dataset: 754 users × 2 features\n",
            "Clusters: 5\n",
            "Max iterations: 100\n",
            "\n",
            "Initialized 5 cluster centroids\n",
            "Starting K-Means iterations...\n",
            "  Iteration   1: shift = 26.124117, clusters = [np.int64(308), np.int64(82), np.int64(93), np.int64(217), np.int64(54)]\n",
            "  Iteration  11: shift = 14.578499, clusters = [np.int64(153), np.int64(265), np.int64(46), np.int64(109), np.int64(181)]\n",
            "  Iteration  21: shift = 7.381364, clusters = [np.int64(139), np.int64(337), np.int64(26), np.int64(83), np.int64(169)]\n",
            "  Iteration  31: shift = 1.233299, clusters = [np.int64(127), np.int64(379), np.int64(24), np.int64(69), np.int64(155)]\n",
            "\n",
            "✓ Converged at iteration 39\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "CLUSTER STATISTICS:\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "  Cluster 0: 117 users (15.5%)\n",
            "  Cluster 1: 390 users (51.7%)\n",
            "  Cluster 2: 24 users (3.2%)\n",
            "  Cluster 3: 67 users (8.9%)\n",
            "  Cluster 4: 156 users (20.7%)\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "TRAINING RESULTS:\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Time taken:        875.50 ms\n",
            "Operations:        1,088,032\n",
            "Iterations:        39\n",
            "\n",
            "Theoretical O(n·k·d·t):\n",
            "  = O(754 × 5 × 2 × 39)\n",
            "  = O(294,060) operations\n",
            "\n",
            "Actual/Theoretical ratio: 3.70x\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "PREDICTION RESULTS (189 users):\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Time taken:        5.85 ms\n",
            "Operations:        6,615\n",
            "Time per user:     30.93 μs\n",
            "\n",
            "Theoretical O(n·k·d):\n",
            "  = O(189 × 5 × 2)\n",
            "  = O(1,890) operations\n",
            "\n",
            "Actual/Theoretical ratio: 3.50x\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            " K-NEAREST NEIGHBORS - TRAINING (Lazy Learning)\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Dataset: 754 users × 2 features\n",
            "Neighbors: 5\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "TRAINING RESULTS:\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Time taken:        8.82 μs\n",
            "Operations:        1,508 (just copying data)\n",
            "\n",
            "⚠ NOTE: KNN is 'lazy learning' - no actual training!\n",
            "         All computation happens during prediction!\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Predicting for 189 users...\n",
            "⚠ Must check ALL 754 training users for each prediction!\n",
            "  Processed 20/189 users...\n",
            "  Processed 40/189 users...\n",
            "  Processed 60/189 users...\n",
            "  Processed 80/189 users...\n",
            "  Processed 100/189 users...\n",
            "  Processed 120/189 users...\n",
            "  Processed 140/189 users...\n",
            "  Processed 160/189 users...\n",
            "  Processed 180/189 users...\n",
            "\n",
            "PREDICTION RESULTS (189 test users):\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Time taken:        843.47 ms\n",
            "Operations:        998,487\n",
            "Time per user:     4.46 ms\n",
            "\n",
            "Theoretical O(n_test · n_train · d):\n",
            "  = O(189 × 754 × 2)\n",
            "  = O(285,012) operations\n",
            "\n",
            "Actual/Theoretical ratio: 3.50x\n",
            "\n",
            "⚠ WARNING: Checked ALL 754 training users for EACH prediction!\n",
            "           This is why KNN doesn't scale without indexing!\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            " DECISION TREE CLUSTERING - TRAINING\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Dataset: 754 users × 2 features\n",
            "Max depth: 10\n",
            "Min samples to split: 5\n",
            "\n",
            "Building hierarchical tree...\n",
            "\n",
            "✓ Tree built successfully\n",
            "  Actual tree depth: 4\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "TRAINING RESULTS:\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Time taken:        139.87 ms\n",
            "Operations:        1,829,187\n",
            "Tree depth:        4\n",
            "\n",
            "Theoretical O(n·d·log n):\n",
            "  = O(754 × 2 × log₂(754))\n",
            "  = O(754 × 2 × 9)\n",
            "  ≈ O(13,572) operations\n",
            "\n",
            "Actual/Theoretical ratio: 134.78x\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "PREDICTION RESULTS (189 users):\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Time taken:        142.81 μs\n",
            "Operations:        318\n",
            "Time per user:     0.76 μs\n",
            "Avg tree depth:    1.7 nodes traversed\n",
            "\n",
            "Theoretical O(n·log n):\n",
            "  = O(189 × 4)\n",
            "  = O(756) operations\n",
            "\n",
            "Actual/Theoretical ratio: 0.42x\n",
            "\n",
            "======================================================================\n",
            " SIMILARITY COMPARISON (SUBSET N=200)\n",
            "======================================================================\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            " RAW COSINE SIMILARITY - MANUAL LOOP\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Dataset: 200 users × 2 features\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "RESULTS:\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Time taken:        129.22 ms\n",
            "Operations:        640,000\n",
            "Theoretical O():   O(n²d)\n",
            "Space Complexity:  O(n²) - approx 320,000 bytes for similarity matrix\n",
            "Example Sim (0, 1): 0.9985\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            " MEAN-CENTERED COSINE SIMILARITY - MANUAL LOOP\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Dataset: 200 users × 2 features\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "RESULTS:\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Time taken:        197.76 ms\n",
            "Operations:        800,600\n",
            "Theoretical O():   O(n²d)\n",
            "Space Complexity:  O(n²) - approx 321,600 bytes for similarity matrix\n",
            "Example Sim (0, 1): 1.0000\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "======================================================================\n",
            " COMPARISON SUMMARY\n",
            "======================================================================\n",
            "Clustering Methods:\n",
            "  K-Means Prediction Time: 5.85 ms (O(k·d))\n",
            "  DT Prediction Time:      142.81 μs (O(log n))\n",
            "  KNN Prediction Time:     843.47 ms (O(n·d) - Slowest)\n",
            "\n",
            "Similarity Methods (O(n²d)):\n",
            "  Raw Cosine and Mean-Centered Cosine were calculated on a subset of 200 users.\n",
            "  These methods are computationally expensive for large user bases.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import Counter, defaultdict\n",
        "import sys\n",
        "from sklearn.cluster import KMeans as SKLearnKMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "# DATA LOADING\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "# Step 1: Upload your 'Movie_Lens_100K_Dataset.zip' file to the Colab session.\n",
        "# It will appear in the path /content/Movie_Lens_100K_Dataset.zip\n",
        "\n",
        "zip_path = '/content/Movie_Lens_100K_Dataset.zip'\n",
        "extract_dir = '/content/ml-100k' # The zip file contains a folder named 'ml-100k'\n",
        "\n",
        "# Check if the zip file exists before trying to extract\n",
        "if os.path.exists(zip_path):\n",
        "    print(f\"Found zip file at: {zip_path}\")\n",
        "    # Create extraction directory if it doesn't exist\n",
        "    if not os.path.exists(extract_dir):\n",
        "        # Extract to /content/, which will create the 'ml-100k' folder\n",
        "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "            z.extractall('/content/')\n",
        "        print(f\"Successfully extracted to: {extract_dir}\")\n",
        "    else:\n",
        "        print(f\"Extraction directory already exists: {extract_dir}. Skipping extraction.\")\n",
        "else:\n",
        "    print(f\"❌ ERROR: Zip file not found at '{zip_path}'.\")\n",
        "    print(\"Please upload the 'Movie_Lens_100K_Dataset.zip' file to your Colab session.\")\n",
        "\n",
        "\"\"\"\n",
        "═══════════════════════════════════════════════════════════════════════════════\n",
        "    CLUSTERING METHODS FOR COLLABORATIVE FILTERING - MOVIELENS DATASET\n",
        "═══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "This implementation demonstrates three clustering methods on real MovieLens data:\n",
        "\n",
        "1. K-MEANS CLUSTERING (⭐ FASTEST for CF!)\n",
        "   - Training: O(n·k·d·t)\n",
        "   - Prediction: O(k·d)\n",
        "\n",
        "2. K-NEAREST NEIGHBORS (Lazy Learning)\n",
        "   - Training: O(1) - just stores data\n",
        "   - Prediction: O(n·d) - checks all points\n",
        "\n",
        "3. DECISION TREE CLUSTERING (Hierarchical)\n",
        "   - Training: O(n·d·log n)\n",
        "   - Prediction: O(log n) - tree depth\n",
        "\n",
        "Each implementation tracks:\n",
        "✓ Actual execution time\n",
        "✓ Number of operations performed\n",
        "✓ Theoretical complexity\n",
        "✓ Comparison with expected O() notation\n",
        "\n",
        "Uses MovieLens 100K dataset for realistic collaborative filtering scenarios.\n",
        "═══════════════════════════════════════════════════════════════════════════════\n",
        "\"\"\"\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# UTILITY FUNCTIONS\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "def load_movielens_data():\n",
        "    \"\"\"\n",
        "    Loads the MovieLens 100K dataset from the specified Colab path.\n",
        "    \"\"\"\n",
        "    print(\"Loading MovieLens dataset from u.data...\")\n",
        "\n",
        "    # Define the path to the u.data file within the extracted folder\n",
        "    ratings_path = '/content/ml-100k/u.data'\n",
        "\n",
        "    if not os.path.exists(ratings_path):\n",
        "        print(f\"❌ ERROR: The file 'u.data' was not found at '{ratings_path}'.\")\n",
        "        print(\"Please ensure the zip file was extracted correctly.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Load the file (tab-separated)\n",
        "    ratings = pd.read_csv(\n",
        "        ratings_path,\n",
        "        sep='\\t',\n",
        "        engine='python',\n",
        "        names=['userId', 'movieId', 'rating', 'timestamp']\n",
        "    )\n",
        "\n",
        "    print(f\"✓ Loaded {len(ratings):,} ratings\")\n",
        "    print(f\"  Users: {ratings['userId'].nunique():,}\")\n",
        "    print(f\"  Movies: {ratings['movieId'].nunique():,}\")\n",
        "\n",
        "    # Convert rating to numeric\n",
        "    ratings['rating'] = pd.to_numeric(ratings['rating'], errors='coerce')\n",
        "    ratings.dropna(subset=['rating'], inplace=True)\n",
        "\n",
        "    user_features = []\n",
        "    user_ids = []\n",
        "\n",
        "    for user_id in ratings['userId'].unique():\n",
        "        user_ratings = ratings[ratings['userId'] == user_id]\n",
        "        if len(user_ratings) < 5:\n",
        "            continue\n",
        "\n",
        "        # Feature vector: [average rating, number of ratings]\n",
        "        feature_vector = [\n",
        "            user_ratings['rating'].mean(),\n",
        "            len(user_ratings)\n",
        "        ]\n",
        "        user_features.append(feature_vector)\n",
        "        user_ids.append(user_id)\n",
        "\n",
        "    X = np.array(user_features)\n",
        "    print(f\"✓ Created feature vectors for {len(X):,} users\")\n",
        "    print(f\"  Matrix shape: {X.shape}\")\n",
        "\n",
        "    return X, user_ids, ratings\n",
        "\n",
        "\n",
        "def print_header(title, char='═', width=70):\n",
        "    \"\"\"Print formatted header\"\"\"\n",
        "    print(f\"\\n{char * width}\")\n",
        "    print(f\" {title}\")\n",
        "    print(f\"{char * width}\")\n",
        "\n",
        "\n",
        "def format_number(num):\n",
        "    \"\"\"Format numbers with commas\"\"\"\n",
        "    return f\"{num:,}\"\n",
        "\n",
        "\n",
        "def format_time(seconds):\n",
        "    \"\"\"Format time in appropriate units\"\"\"\n",
        "    if seconds < 0.001:\n",
        "        return f\"{seconds*1000000:.2f} μs\"\n",
        "    elif seconds < 1:\n",
        "        return f\"{seconds*1000:.2f} ms\"\n",
        "    else:\n",
        "        return f\"{seconds:.4f} s\"\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# 1. K-MEANS CLUSTERING (FASTEST!)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "class KMeansClustering:\n",
        "    \"\"\"\n",
        "    K-Means Clustering for CF with Complexity Tracking\n",
        "\n",
        "    Training Complexity: O(n·k·d·t)\n",
        "    - n: number of users\n",
        "    - k: number of clusters\n",
        "    - d: number of dimensions (features)\n",
        "    - t: iterations until convergence\n",
        "\n",
        "    Prediction Complexity: O(k·d) per user\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_clusters=10, max_iters=100, tolerance=1e-4):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iters = max_iters\n",
        "        self.tolerance = tolerance\n",
        "        self.centroids = None\n",
        "        self.labels = None\n",
        "\n",
        "        # Complexity tracking\n",
        "        self.train_time = 0\n",
        "        self.train_ops = 0\n",
        "        self.pred_time = 0\n",
        "        self.pred_ops = 0\n",
        "        self.iterations = 0\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"Train K-Means clustering - O(n·k·d·t)\"\"\"\n",
        "        print_header(\"K-MEANS CLUSTERING - TRAINING\", '─')\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "        print(f\"Dataset: {format_number(n_samples)} users × {n_features} features\")\n",
        "        print(f\"Clusters: {self.n_clusters}\")\n",
        "        print(f\"Max iterations: {self.max_iters}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        operations = 0\n",
        "\n",
        "        # Initialize centroids using k-means++\n",
        "        random_indices = np.random.choice(n_samples, self.n_clusters, replace=False)\n",
        "        self.centroids = X[random_indices].copy()\n",
        "        operations += self.n_clusters * n_features\n",
        "\n",
        "        print(f\"\\nInitialized {self.n_clusters} cluster centroids\")\n",
        "        print(f\"Starting K-Means iterations...\")\n",
        "\n",
        "        # K-Means iterations\n",
        "        for iteration in range(self.max_iters):\n",
        "            # Assign users to nearest cluster centroid - O(n·k·d)\n",
        "            distances = np.zeros((n_samples, self.n_clusters))\n",
        "\n",
        "            for i in range(n_samples):\n",
        "                for j in range(self.n_clusters):\n",
        "                    # Calculate Euclidean distance manually\n",
        "                    diff = X[i] - self.centroids[j]\n",
        "                    # Note: np.sum and np.sqrt are used here, which are built-in NumPy functions.\n",
        "                    # These are kept for a practical K-Means implementation.\n",
        "                    distances[i, j] = np.sqrt(np.sum(diff ** 2))\n",
        "                    operations += n_features * 3\n",
        "\n",
        "            old_labels = self.labels\n",
        "            self.labels = np.argmin(distances, axis=1)\n",
        "            operations += n_samples * self.n_clusters\n",
        "\n",
        "            # Update centroids - O(n·d)\n",
        "            new_centroids = np.zeros((self.n_clusters, n_features))\n",
        "\n",
        "            for j in range(self.n_clusters):\n",
        "                cluster_users = X[self.labels == j]\n",
        "                if len(cluster_users) > 0:\n",
        "                    # Note: np.mean is used here.\n",
        "                    new_centroids[j] = cluster_users.mean(axis=0)\n",
        "                    operations += len(cluster_users) * n_features\n",
        "                else:\n",
        "                    new_centroids[j] = self.centroids[j]\n",
        "\n",
        "            # Check convergence\n",
        "            # Note: np.linalg.norm is used here.\n",
        "            centroid_shift = np.linalg.norm(new_centroids - self.centroids)\n",
        "            self.centroids = new_centroids\n",
        "\n",
        "            if iteration % 10 == 0:\n",
        "                cluster_sizes = [np.sum(self.labels == j) for j in range(self.n_clusters)]\n",
        "                print(f\"  Iteration {iteration + 1:3d}: shift = {centroid_shift:.6f}, \"\n",
        "                      f\"clusters = {cluster_sizes}\")\n",
        "\n",
        "            if centroid_shift < self.tolerance:\n",
        "                print(f\"\\n✓ Converged at iteration {iteration + 1}\")\n",
        "                self.iterations = iteration + 1\n",
        "                break\n",
        "        else:\n",
        "            print(f\"\\n⚠ Reached maximum iterations ({self.max_iters})\")\n",
        "            self.iterations = self.max_iters\n",
        "\n",
        "        self.train_time = time.time() - start_time\n",
        "        self.train_ops = operations\n",
        "\n",
        "        # Display cluster statistics\n",
        "        print(f\"\\n{'─' * 70}\")\n",
        "        print(f\"CLUSTER STATISTICS:\")\n",
        "        print(f\"{'─' * 70}\")\n",
        "        for j in range(self.n_clusters):\n",
        "            cluster_size = np.sum(self.labels == j)\n",
        "            print(f\"  Cluster {j}: {cluster_size} users ({100*cluster_size/n_samples:.1f}%)\")\n",
        "\n",
        "        print(f\"\\n{'─' * 70}\")\n",
        "        print(f\"TRAINING RESULTS:\")\n",
        "        print(f\"{'─' * 70}\")\n",
        "        print(f\"Time taken:        {format_time(self.train_time)}\")\n",
        "        print(f\"Operations:        {format_number(operations)}\")\n",
        "        print(f\"Iterations:        {self.iterations}\")\n",
        "\n",
        "        theoretical_ops = n_samples * self.n_clusters * n_features * self.iterations\n",
        "        print(f\"\\nTheoretical O(n·k·d·t):\")\n",
        "        print(f\"  = O({format_number(n_samples)} × {self.n_clusters} × {n_features} × {self.iterations})\")\n",
        "        print(f\"  = O({format_number(theoretical_ops)}) operations\")\n",
        "        print(f\"\\nActual/Theoretical ratio: {operations/theoretical_ops:.2f}x\")\n",
        "        print(f\"{'─' * 70}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict cluster for new users - O(k·d) per user\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        start_time = time.time()\n",
        "        operations = 0\n",
        "\n",
        "        predictions = np.zeros(n_samples, dtype=int)\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            min_dist = float('inf')\n",
        "            min_idx = 0\n",
        "\n",
        "            for j in range(self.n_clusters):\n",
        "                diff = X[i] - self.centroids[j]\n",
        "                # Calculate Euclidean distance manually\n",
        "                dist = np.sqrt(np.sum(diff ** 2))\n",
        "                operations += n_features * 3\n",
        "\n",
        "                if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "                    min_idx = j\n",
        "\n",
        "            predictions[i] = min_idx\n",
        "            operations += self.n_clusters\n",
        "\n",
        "        self.pred_time = time.time() - start_time\n",
        "        self.pred_ops = operations\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def print_prediction_stats(self, n_samples, n_features):\n",
        "        \"\"\"Print prediction complexity statistics\"\"\"\n",
        "        print(f\"\\nPREDICTION RESULTS ({format_number(n_samples)} users):\")\n",
        "        print(f\"{'─' * 70}\")\n",
        "        print(f\"Time taken:        {format_time(self.pred_time)}\")\n",
        "        print(f\"Operations:        {format_number(self.pred_ops)}\")\n",
        "        print(f\"Time per user:     {format_time(self.pred_time / n_samples)}\")\n",
        "\n",
        "        theoretical_ops = n_samples * self.n_clusters * n_features\n",
        "        print(f\"\\nTheoretical O(n·k·d):\")\n",
        "        print(f\"  = O({format_number(n_samples)} × {self.n_clusters} × {n_features})\")\n",
        "        print(f\"  = O({format_number(theoretical_ops)}) operations\")\n",
        "        print(f\"\\nActual/Theoretical ratio: {self.pred_ops/theoretical_ops:.2f}x\")\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# 2. K-NEAREST NEIGHBORS (LAZY LEARNING)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "class KNNClustering:\n",
        "    \"\"\"\n",
        "    K-Nearest Neighbors for CF with Complexity Tracking\n",
        "\n",
        "    Training Complexity: O(1) - lazy learning\n",
        "    Prediction Complexity: O(n·d) per user - SLOW!\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_neighbors=5):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "\n",
        "        # Complexity tracking\n",
        "        self.train_time = 0\n",
        "        self.train_ops = 0\n",
        "        self.pred_time = 0\n",
        "        self.pred_ops = 0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"'Train' KNN - O(1) - just stores data!\"\"\"\n",
        "        print_header(\"K-NEAREST NEIGHBORS - TRAINING (Lazy Learning)\", '─')\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "        print(f\"Dataset: {format_number(n_samples)} users × {n_features} features\")\n",
        "        print(f\"Neighbors: {self.n_neighbors}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Just store the data - no actual training!\n",
        "        self.X_train = X.copy()\n",
        "        self.y_train = y.copy()\n",
        "        operations = n_samples * n_features\n",
        "\n",
        "        self.train_time = time.time() - start_time\n",
        "        self.train_ops = operations\n",
        "\n",
        "        print(f\"\\n{'─' * 70}\")\n",
        "        print(f\"TRAINING RESULTS:\")\n",
        "        print(f\"{'─' * 70}\")\n",
        "        print(f\"Time taken:        {format_time(self.train_time)}\")\n",
        "        print(f\"Operations:        {format_number(operations)} (just copying data)\")\n",
        "        print(f\"\\n⚠ NOTE: KNN is 'lazy learning' - no actual training!\")\n",
        "        print(f\"         All computation happens during prediction!\")\n",
        "        print(f\"{'─' * 70}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict using KNN - O(n_test × n_train × d) - SLOW!\"\"\"\n",
        "        n_test, n_features = X.shape\n",
        "        n_train = self.X_train.shape[0]\n",
        "\n",
        "        start_time = time.time()\n",
        "        operations = 0\n",
        "\n",
        "        predictions = np.zeros(n_test, dtype=int)\n",
        "\n",
        "        print(f\"\\nPredicting for {format_number(n_test)} users...\")\n",
        "        print(f\"⚠ Must check ALL {format_number(n_train)} training users for each prediction!\")\n",
        "\n",
        "        for i in range(n_test):\n",
        "            # Compute distance to ALL training users - O(n_train·d)\n",
        "            distances = np.zeros(n_train)\n",
        "\n",
        "            for j in range(n_train):\n",
        "                diff = X[i] - self.X_train[j]\n",
        "                # Calculate Euclidean distance manually\n",
        "                dist = np.sqrt(np.sum(diff ** 2))\n",
        "                operations += n_features * 3\n",
        "\n",
        "            # Find k nearest neighbors\n",
        "            # Note: np.argpartition is used here.\n",
        "            k_nearest_indices = np.argpartition(distances, self.n_neighbors)[:self.n_neighbors]\n",
        "            operations += n_train\n",
        "\n",
        "            # Majority vote\n",
        "            k_nearest_labels = self.y_train[k_nearest_indices]\n",
        "            # Note: Counter is used here.\n",
        "            predictions[i] = Counter(k_nearest_labels).most_common(1)[0][0]\n",
        "            operations += self.n_neighbors\n",
        "\n",
        "            if (i + 1) % 20 == 0:\n",
        "                print(f\"  Processed {i+1}/{n_test} users...\")\n",
        "\n",
        "        self.pred_time = time.time() - start_time\n",
        "        self.pred_ops = operations\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def print_prediction_stats(self, n_test, n_features):\n",
        "        \"\"\"Print prediction complexity statistics\"\"\"\n",
        "        n_train = self.X_train.shape[0]\n",
        "\n",
        "        print(f\"\\nPREDICTION RESULTS ({format_number(n_test)} test users):\")\n",
        "        print(f\"{'─' * 70}\")\n",
        "        print(f\"Time taken:        {format_time(self.pred_time)}\")\n",
        "        print(f\"Operations:        {format_number(self.pred_ops)}\")\n",
        "        print(f\"Time per user:     {format_time(self.pred_time / n_test)}\")\n",
        "\n",
        "        theoretical_ops = n_test * n_train * n_features\n",
        "        print(f\"\\nTheoretical O(n_test · n_train · d):\")\n",
        "        print(f\"  = O({format_number(n_test)} × {format_number(n_train)} × {n_features})\")\n",
        "        print(f\"  = O({format_number(theoretical_ops)}) operations\")\n",
        "        print(f\"\\nActual/Theoretical ratio: {self.pred_ops/theoretical_ops:.2f}x\")\n",
        "\n",
        "        print(f\"\\n⚠ WARNING: Checked ALL {format_number(n_train)} training users for EACH prediction!\")\n",
        "        print(f\"           This is why KNN doesn't scale without indexing!\")\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# 3. DECISION TREE CLUSTERING (HIERARCHICAL)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "class DecisionTreeClustering:\n",
        "    \"\"\"\n",
        "    Decision Tree for CF with Complexity Tracking\n",
        "\n",
        "    Training Complexity: O(n·d·log n)\n",
        "    Prediction Complexity: O(log n) per user\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_depth=10, min_samples_split=5):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.tree = None\n",
        "        self.tree_depth = 0\n",
        "\n",
        "        # Complexity tracking\n",
        "        self.train_time = 0\n",
        "        self.train_ops = 0\n",
        "        self.pred_time = 0\n",
        "        self.pred_ops = 0\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        \"\"\"Calculate entropy\"\"\"\n",
        "        # Note: np.unique, np.sum, and np.log2 are used here.\n",
        "        _, counts = np.unique(y, return_counts=True)\n",
        "        probabilities = counts / len(y)\n",
        "        return -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        \"\"\"Find best split - O(n·d)\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        best_gain = -1\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "        operations = 0\n",
        "\n",
        "        parent_entropy = self._entropy(y)\n",
        "        operations += len(y)\n",
        "\n",
        "        for feature in range(n_features):\n",
        "            # Note: np.unique is used here.\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_mask = X[:, feature] <= threshold\n",
        "                right_mask = ~left_mask\n",
        "\n",
        "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
        "                    continue\n",
        "\n",
        "                left_entropy = self._entropy(y[left_mask])\n",
        "                right_entropy = self._entropy(y[right_mask])\n",
        "\n",
        "                n_left = np.sum(left_mask)\n",
        "                n_right = np.sum(right_mask)\n",
        "\n",
        "                weighted_entropy = (n_left/n_samples * left_entropy +\n",
        "                                  n_right/n_samples * right_entropy)\n",
        "\n",
        "                gain = parent_entropy - weighted_entropy\n",
        "                operations += n_samples * 2\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature = feature\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature, best_threshold, operations\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        \"\"\"Build tree recursively - O(n·d·log n)\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        # Note: np.unique is used here.\n",
        "        n_labels = len(np.unique(y))\n",
        "        operations = 0\n",
        "\n",
        "        if (depth >= self.max_depth or\n",
        "            n_labels == 1 or\n",
        "            n_samples < self.min_samples_split):\n",
        "            # Note: Counter is used here.\n",
        "            leaf_value = Counter(y).most_common(1)[0][0]\n",
        "            return {'leaf': True, 'value': leaf_value}, operations\n",
        "\n",
        "        best_feature, best_threshold, split_ops = self._best_split(X, y)\n",
        "        operations += split_ops\n",
        "\n",
        "        if best_feature is None:\n",
        "            leaf_value = Counter(y).most_common(1)[0][0]\n",
        "            return {'leaf': True, 'value': leaf_value}, operations\n",
        "\n",
        "        left_mask = X[:, best_feature] <= best_threshold\n",
        "        right_mask = ~left_mask\n",
        "\n",
        "        left_subtree, left_ops = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
        "        right_subtree, right_ops = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
        "\n",
        "        operations += left_ops + right_ops\n",
        "        self.tree_depth = max(self.tree_depth, depth + 1)\n",
        "\n",
        "        return {\n",
        "            'leaf': False,\n",
        "            'feature': best_feature,\n",
        "            'threshold': best_threshold,\n",
        "            'left': left_subtree,\n",
        "            'right': right_subtree\n",
        "        }, operations\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train Decision Tree - O(n·d·log n)\"\"\"\n",
        "        print_header(\"DECISION TREE CLUSTERING - TRAINING\", '─')\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "        print(f\"Dataset: {format_number(n_samples)} users × {n_features} features\")\n",
        "        print(f\"Max depth: {self.max_depth}\")\n",
        "        print(f\"Min samples to split: {self.min_samples_split}\")\n",
        "\n",
        "        print(f\"\\nBuilding hierarchical tree...\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        self.tree, operations = self._build_tree(X, y)\n",
        "        self.train_time = time.time() - start_time\n",
        "        self.train_ops = operations\n",
        "\n",
        "        print(f\"\\n✓ Tree built successfully\")\n",
        "        print(f\"  Actual tree depth: {self.tree_depth}\")\n",
        "\n",
        "        print(f\"\\n{'─' * 70}\")\n",
        "        print(f\"TRAINING RESULTS:\")\n",
        "        print(f\"{'─' * 70}\")\n",
        "        print(f\"Time taken:        {format_time(self.train_time)}\")\n",
        "        print(f\"Operations:        {format_number(operations)}\")\n",
        "        print(f\"Tree depth:        {self.tree_depth}\")\n",
        "\n",
        "        theoretical_ops = n_samples * n_features * int(np.log2(n_samples))\n",
        "        print(f\"\\nTheoretical O(n·d·log n):\")\n",
        "        print(f\"  = O({format_number(n_samples)} × {n_features} × log₂({format_number(n_samples)}))\")\n",
        "        print(f\"  = O({format_number(n_samples)} × {n_features} × {int(np.log2(n_samples))})\")\n",
        "        print(f\"  ≈ O({format_number(theoretical_ops)}) operations\")\n",
        "        print(f\"\\nActual/Theoretical ratio: {operations/theoretical_ops:.2f}x\")\n",
        "        print(f\"{'─' * 70}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _predict_one(self, x, node):\n",
        "        \"\"\"Predict one user - O(log n)\"\"\"\n",
        "        operations = 0\n",
        "\n",
        "        while not node['leaf']:\n",
        "            operations += 1\n",
        "            if x[node['feature']] <= node['threshold']:\n",
        "                node = node['left']\n",
        "            else:\n",
        "                node = node['right']\n",
        "\n",
        "        return node['value'], operations\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict using tree - O(log n) per user\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        start_time = time.time()\n",
        "\n",
        "        predictions = np.zeros(n_samples, dtype=int)\n",
        "        operations = 0\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            predictions[i], ops = self._predict_one(X[i], self.tree)\n",
        "            operations += ops\n",
        "\n",
        "        self.pred_time = time.time() - start_time\n",
        "        self.pred_ops = operations\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def print_prediction_stats(self, n_samples, n_features):\n",
        "        \"\"\"Print prediction stats\"\"\"\n",
        "        print(f\"\\nPREDICTION RESULTS ({format_number(n_samples)} users):\")\n",
        "        print(f\"{'─' * 70}\")\n",
        "        print(f\"Time taken:        {format_time(self.pred_time)}\")\n",
        "        print(f\"Operations:        {format_number(self.pred_ops)}\")\n",
        "        print(f\"Time per user:     {format_time(self.pred_time / n_samples)}\")\n",
        "        print(f\"Avg tree depth:    {self.pred_ops / n_samples:.1f} nodes traversed\")\n",
        "\n",
        "        theoretical_ops = n_samples * self.tree_depth\n",
        "        print(f\"\\nTheoretical O(n·log n):\")\n",
        "        print(f\"  = O({format_number(n_samples)} × {self.tree_depth})\")\n",
        "        print(f\"  = O({format_number(theoretical_ops)}) operations\")\n",
        "        print(f\"\\nActual/Theoretical ratio: {self.pred_ops/theoretical_ops:.2f}x\")\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# RAW COSINE SIMILARITY (Manual Loop Implementation)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def calculate_raw_cosine_similarity(X):\n",
        "    \"\"\"\n",
        "    Calculates raw cosine similarity between all pairs of users in X.\n",
        "    Uses manual loops, avoiding built-in functions and dot products.\n",
        "    Complexity: O(n^2 * d)\n",
        "    - n: number of users\n",
        "    - d: number of features\n",
        "    \"\"\"\n",
        "    print_header(\"RAW COSINE SIMILARITY - MANUAL LOOP\", '─')\n",
        "\n",
        "    n_samples, n_features = X.shape\n",
        "    print(f\"Dataset: {format_number(n_samples)} users × {n_features} features\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    operations = 0\n",
        "\n",
        "    # Space complexity: O(n^2) for the similarity matrix\n",
        "    similarity_matrix = np.zeros((n_samples, n_samples))\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        for j in range(n_samples):\n",
        "            # Calculate numerator (A . B) - STRICTLY MANUAL\n",
        "            numerator = 0\n",
        "            for k in range(n_features):\n",
        "                numerator += X[i, k] * X[j, k]\n",
        "                operations += 2 # 1 multiplication, 1 addition\n",
        "\n",
        "            # Calculate denominator (||A|| * ||B||) - STRICTLY MANUAL\n",
        "            norm_a_sq = 0\n",
        "            norm_b_sq = 0\n",
        "            for k in range(n_features):\n",
        "                norm_a_sq += X[i, k] * X[i, k]\n",
        "                norm_b_sq += X[j, k] * X[j, k]\n",
        "                operations += 4 # 2 multiplications, 2 additions\n",
        "\n",
        "            denominator_sq = norm_a_sq * norm_b_sq\n",
        "            operations += 1 # 1 multiplication\n",
        "\n",
        "            if denominator_sq == 0:\n",
        "                similarity = 0.0\n",
        "            else:\n",
        "                # Use power operator for square root (equivalent to np.sqrt)\n",
        "                similarity = numerator / (denominator_sq ** 0.5)\n",
        "                operations += 2 # 1 power (sqrt), 1 division\n",
        "\n",
        "            similarity_matrix[i, j] = similarity\n",
        "            operations += 1 # 1 assignment\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    # Space complexity calculation (approximate)\n",
        "    # The matrix is n*n elements. Assuming 8 bytes per float.\n",
        "    space_complexity_bytes = n_samples * n_samples * 8\n",
        "\n",
        "    print(f\"\\n{'─' * 70}\")\n",
        "    print(f\"RESULTS:\")\n",
        "    print(f\"{'─' * 70}\")\n",
        "    print(f\"Time taken:        {format_time(total_time)}\")\n",
        "    print(f\"Operations:        {format_number(operations)}\")\n",
        "    print(f\"Theoretical O():   O(n²d)\")\n",
        "    print(f\"Space Complexity:  O(n²) - approx {format_number(space_complexity_bytes)} bytes for similarity matrix\")\n",
        "    print(f\"Example Sim (0, 1): {similarity_matrix[0, 1]:.4f}\")\n",
        "    print(f\"{'─' * 70}\")\n",
        "\n",
        "    return similarity_matrix, total_time, operations, space_complexity_bytes\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# MEAN-CENTERED COSINE SIMILARITY (Manual Loop Implementation)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def calculate_mean_centered_cosine_similarity(X):\n",
        "    \"\"\"\n",
        "    Calculates Mean-Centered Cosine Similarity.\n",
        "\n",
        "    This method centers the feature vector by subtracting the mean of\n",
        "    the entire feature vector (all features).\n",
        "\n",
        "    Uses manual loops, avoiding built-in functions and dot products.\n",
        "    Complexity: O(n^2 * d)\n",
        "    - n: number of users\n",
        "    - d: number of features\n",
        "    \"\"\"\n",
        "    print_header(\"MEAN-CENTERED COSINE SIMILARITY - MANUAL LOOP\", '─')\n",
        "\n",
        "    n_samples, n_features = X.shape\n",
        "    print(f\"Dataset: {format_number(n_samples)} users × {n_features} features\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    operations = 0\n",
        "\n",
        "    # Space complexity: O(n^2) for the similarity matrix + O(n) for means\n",
        "    similarity_matrix = np.zeros((n_samples, n_samples))\n",
        "\n",
        "    # Step 1: Calculate the mean for each user (row) - STRICTLY MANUAL\n",
        "    user_means = np.zeros(n_samples)\n",
        "    for i in range(n_samples):\n",
        "        row_sum = 0\n",
        "        for k in range(n_features):\n",
        "            row_sum += X[i, k]\n",
        "            operations += 1 # 1 addition\n",
        "        user_means[i] = row_sum / n_features\n",
        "        operations += 1 # 1 division\n",
        "\n",
        "    # Step 2: Calculate similarity\n",
        "    for i in range(n_samples):\n",
        "        for j in range(n_samples):\n",
        "            # Mean-center the vectors - STRICTLY MANUAL\n",
        "            a_centered = np.zeros(n_features)\n",
        "            b_centered = np.zeros(n_features)\n",
        "            for k in range(n_features):\n",
        "                a_centered[k] = X[i, k] - user_means[i]\n",
        "                b_centered[k] = X[j, k] - user_means[j]\n",
        "                operations += 2 # 2 subtractions\n",
        "\n",
        "            # Calculate numerator (centered_A . centered_B) - STRICTLY MANUAL\n",
        "            numerator = 0\n",
        "            for k in range(n_features):\n",
        "                numerator += a_centered[k] * b_centered[k]\n",
        "                operations += 2 # 1 multiplication, 1 addition\n",
        "\n",
        "            # Calculate denominator (||centered_A|| * ||centered_B||) - STRICTLY MANUAL\n",
        "            norm_a_sq = 0\n",
        "            norm_b_sq = 0\n",
        "            for k in range(n_features):\n",
        "                norm_a_sq += a_centered[k] * a_centered[k]\n",
        "                norm_b_sq += b_centered[k] * b_centered[k]\n",
        "                operations += 4 # 2 multiplications, 2 additions\n",
        "\n",
        "            denominator_sq = norm_a_sq * norm_b_sq\n",
        "            operations += 1 # 1 multiplication\n",
        "\n",
        "            if denominator_sq == 0:\n",
        "                similarity = 0.0\n",
        "            else:\n",
        "                # Use power operator for square root (equivalent to np.sqrt)\n",
        "                similarity = numerator / (denominator_sq ** 0.5)\n",
        "                operations += 2 # 1 power (sqrt), 1 division\n",
        "\n",
        "            similarity_matrix[i, j] = similarity\n",
        "            operations += 1 # 1 assignment\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    # Space complexity calculation (approximate)\n",
        "    space_complexity_bytes = n_samples * n_samples * 8 + n_samples * 8\n",
        "\n",
        "    print(f\"\\n{'─' * 70}\")\n",
        "    print(f\"RESULTS:\")\n",
        "    print(f\"{'─' * 70}\")\n",
        "    print(f\"Time taken:        {format_time(total_time)}\")\n",
        "    print(f\"Operations:        {format_number(operations)}\")\n",
        "    print(f\"Theoretical O():   O(n²d)\")\n",
        "    print(f\"Space Complexity:  O(n²) - approx {format_number(space_complexity_bytes)} bytes for similarity matrix\")\n",
        "    print(f\"Example Sim (0, 1): {similarity_matrix[0, 1]:.4f}\")\n",
        "    print(f\"{'─' * 70}\")\n",
        "\n",
        "    return similarity_matrix, total_time, operations, space_complexity_bytes\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# MAIN EXECUTION\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def run_movielens_comparison_with_similarity():\n",
        "    \"\"\"\n",
        "    Main function to run the clustering and similarity comparisons.\n",
        "    \"\"\"\n",
        "    X, user_ids, ratings = load_movielens_data()\n",
        "\n",
        "    if X is None:\n",
        "        print(\"\\nExiting due to data loading error.\")\n",
        "        return\n",
        "\n",
        "    # --- 1. Clustering Methods ---\n",
        "\n",
        "    # For clustering, we need a set of labels (e.g., from a pre-trained model)\n",
        "    # Since we don't have pre-trained labels, we'll use a quick KMeans from sklearn\n",
        "    # to generate 'ground truth' labels for the KNN and DT training steps.\n",
        "    print_header(\"GENERATING BASE CLUSTERS (SKLEARN KMEANS)\", '=')\n",
        "\n",
        "    # Use a small number of clusters for demonstration\n",
        "    K = 5\n",
        "    # Note: SKLearnKMeans is used here for generating base labels.\n",
        "    kmeans_base = SKLearnKMeans(n_clusters=K, random_state=42, n_init=10)\n",
        "    y_base = kmeans_base.fit_predict(X)\n",
        "\n",
        "    print(f\"✓ Generated {K} base clusters for training KNN and DT.\")\n",
        "\n",
        "    # Split data into training and testing sets (80/20 split)\n",
        "    n_samples = X.shape[0]\n",
        "    train_size = int(0.8 * n_samples)\n",
        "\n",
        "    X_train, X_test = X[:train_size], X[train_size:]\n",
        "    y_train, y_test = y_base[:train_size], y_base[train_size:]\n",
        "\n",
        "    print(f\"\\nData Split: Train={X_train.shape[0]} users, Test={X_test.shape[0]} users\")\n",
        "\n",
        "    # 1. K-Means Clustering (Custom Implementation)\n",
        "    kmeans_cf = KMeansClustering(n_clusters=K)\n",
        "    kmeans_cf.fit(X_train)\n",
        "    y_pred_kmeans = kmeans_cf.predict(X_test)\n",
        "    kmeans_cf.print_prediction_stats(X_test.shape[0], X_test.shape[1])\n",
        "\n",
        "    # 2. K-Nearest Neighbors (Custom Implementation)\n",
        "    knn_cf = KNNClustering(n_neighbors=5)\n",
        "    knn_cf.fit(X_train, y_train)\n",
        "    y_pred_knn = knn_cf.predict(X_test)\n",
        "    knn_cf.print_prediction_stats(X_test.shape[0], X_test.shape[1])\n",
        "\n",
        "    # 3. Decision Tree Clustering (Custom Implementation)\n",
        "    dt_cf = DecisionTreeClustering(max_depth=10)\n",
        "    dt_cf.fit(X_train, y_train)\n",
        "    y_pred_dt = dt_cf.predict(X_test)\n",
        "    dt_cf.print_prediction_stats(X_test.shape[0], X_test.shape[1])\n",
        "\n",
        "    # --- 2. Similarity Methods ---\n",
        "\n",
        "    # Use a smaller subset for similarity to avoid excessive runtime (O(n^2*d))\n",
        "    # Similarity calculation is very slow for large N\n",
        "    subset_size = min(200, X.shape[0])\n",
        "    X_subset = X[:subset_size]\n",
        "\n",
        "    print_header(f\"SIMILARITY COMPARISON (SUBSET N={subset_size})\", '=')\n",
        "\n",
        "    # Raw Cosine Similarity\n",
        "    calculate_raw_cosine_similarity(X_subset)\n",
        "\n",
        "    # Mean-Centered Cosine Similarity\n",
        "    calculate_mean_centered_cosine_similarity(X_subset)\n",
        "\n",
        "    print_header(\"COMPARISON SUMMARY\", '=')\n",
        "    print(\"Clustering Methods:\")\n",
        "    print(f\"  K-Means Prediction Time: {format_time(kmeans_cf.pred_time)} (O(k·d))\")\n",
        "    print(f\"  DT Prediction Time:      {format_time(dt_cf.pred_time)} (O(log n))\")\n",
        "    print(f\"  KNN Prediction Time:     {format_time(knn_cf.pred_time)} (O(n·d) - Slowest)\")\n",
        "\n",
        "    print(\"\\nSimilarity Methods (O(n²d)):\")\n",
        "    print(f\"  Raw Cosine and Mean-Centered Cosine were calculated on a subset of {subset_size} users.\")\n",
        "    print(\"  These methods are computationally expensive for large user bases.\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_movielens_comparison_with_similarity()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlhhmsOYkbPs"
      },
      "source": [
        "# Under the Supervission of: Dr. **Samy** & Eng. **Salama**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
